name: app-build-and-deploy

trigger:
  branches:
    include:
      - main  
  paths:
    include:
      - my-app/*

variables:
  - group: aws-keys
  - name: AWS_REGION
    value: us-east-2 # This value was modified by the initial-setup python script
  - name: APP_NAME
    value: viola # This value was modified by the initial-setup python script
  - name: DOCKERHUB_USERNAME
    value: tferrari92 # This value was modified by the initial-setup python script
  - name: USER_EMAIL
    value: tomas.ferrari@sendati.com # This value was modified by the initial-setup python script

pool:
  vmImage: 'ubuntu-latest' 
  # If you are using a self-hosted agent, comment out the previous line and uncomment the following three
  # name: <agent-pool-name> # Insert here the name of the agent pool you created
  # demands:
  #   - agent.name -equals <agent-name> # Insert here the name of the agent you created

stages:
- stage: Build 
  jobs:
  - job: BuildJob
    displayName: 'Build Job'
    steps:
    - task: DockerInstaller@0
      displayName: Install Docker
      inputs:
        dockerVersion: '17.09.0-ce'

    - task: Docker@2
      displayName: Build And Push Image
      inputs:
        containerRegistry: 'dockerhub'
        repository: '$(DOCKERHUB_USERNAME)/$(APP_NAME)'
        command: 'buildAndPush'
        Dockerfile: 'my-app/Dockerfile'
        


- stage: DeployDev
  condition: succeeded()
  jobs:
  - job: DeployDevJob
    displayName: 'Deploy Dev'
    steps:

    # In this case it's necessary to specify the checkout with the persistCredential options set to true. This will enable us to push the changes to the repo.
    - checkout: self
      persistCredentials: true

    - script: |
        sed 's/tag:.*/tag: $(Build.BuildId)/g' helm/my-app/values.yaml > helm/my-app/values.temp
        mv helm/my-app/values.temp helm/my-app/values.yaml
      displayName: Update Tag In values.yaml

    - script: |
        git config --global user.email "AzureDevOps@Build&DeployAppPipeline.com"
        git config --global user.name "Azure DevOps - Build & Deploy App Pipeline"
      displayName: 'Configure Git user'

    - script: |
        git checkout -b main
        git add helm/my-app/values.yaml 
        git commit -m "App version tag updated to $(Build.BuildId) by Azure DevOps"
        git push --set-upstream origin main
      displayName: 'Push changes to GitHub'

    - script: |
        mkdir ~/.aws
        echo -e "[default]\naws_access_key_id = $(aws_access_key_id)\naws_secret_access_key = $(aws_secret_access_key)" > ~/.aws/credentials
        echo -e "[default]\nregion = $(AWS_REGION)"> ~/.aws/config 
      displayName: 'Configure AWS Profile'

    - script: |
        echo "false" > is-active.txt
        while [ "$(cat is-active.txt)" != "true" ]; do
            sleep 5
            aws elbv2 describe-load-balancers > lb-status.json
            json_data=$(cat lb-status.json)
            state_code=$(echo "$json_data" | jq -r '.LoadBalancers[1].State.Code')
            if [[ "$state_code" == "active" ]]; then
              echo "true" > is-active.txt
            else
              echo "Load balancer is not ready yet..."
            fi
        done
      displayName: 'Wait for Load Balancer Status'

    - task: AWSCLI@1
      displayName: 'Update KubeConfig'
      inputs:
        awsCredentials: 'aws'
        regionName: '$(AWS_REGION)' 
        awsCommand: 'eks'
        awsSubCommand: 'update-kubeconfig'
        awsArguments: '--name $(APP_NAME)-cluster --region $(AWS_REGION)' 

    - script: |
        kubectl get ingress -n $(APP_NAME) $(kubectl get ingress -n $(APP_NAME) | awk 'NR>1{print $1}') -o=jsonpath="{'http://'}{.status.loadBalancer.ingress[].hostname}{'\n'}" > my-app-url.txt
      displayName: 'Save URL'

    - task: PublishBuildArtifacts@1
      displayName: 'Export URL'
      inputs:
        PathtoPublish: 'my-app-url.txt'
        ArtifactName: 'URL'
        publishLocation: 'Container'
        



- stage: DeployTest
  condition: succeeded()
  # dependsOn: DeployDev
  jobs:
  - job: DeployTestJob
    displayName: 'Deploy Test'
    steps:

    # In this case it's necessary to specify the checkout with the persistCredential options set to true. This will enable us to push the changes to the repo.
    - checkout: self
      persistCredentials: true

    - script: |
        sed 's/tag:.*/tag: $(Build.BuildId)/g' helm/my-app/values.yaml > helm/my-app/values.temp
        mv helm/my-app/values.temp helm/my-app/values.yaml
      displayName: Update Tag In values.yaml

    - script: |
        git config --global user.email "AzureDevOps@Build&DeployAppPipeline.com"
        git config --global user.name "Azure DevOps - Build & Deploy App Pipeline"
      displayName: 'Configure Git user'

    - script: |
        git checkout -b main
        git add helm/my-app/values.yaml 
        git commit -m "App version tag updated to $(Build.BuildId) by Azure DevOps"
        git push --set-upstream origin main
      displayName: 'Push changes to GitHub'

    - script: |
        mkdir ~/.aws
        echo -e "[default]\naws_access_key_id = $(aws_access_key_id)\naws_secret_access_key = $(aws_secret_access_key)" > ~/.aws/credentials
        echo -e "[default]\nregion = $(AWS_REGION)"> ~/.aws/config 
      displayName: 'Configure AWS Profile'

    - script: |
        echo "false" > is-active.txt
        while [ "$(cat is-active.txt)" != "true" ]; do
            sleep 5
            aws elbv2 describe-load-balancers > lb-status.json
            json_data=$(cat lb-status.json)
            state_code=$(echo "$json_data" | jq -r '.LoadBalancers[1].State.Code')
            if [[ "$state_code" == "active" ]]; then
              echo "true" > is-active.txt
            else
              echo "Load balancer is not ready yet..."
            fi
        done
      displayName: 'Wait for Load Balancer Status'

    - task: AWSCLI@1
      displayName: 'Update KubeConfig'
      inputs:
        awsCredentials: 'aws'
        regionName: '$(AWS_REGION)' 
        awsCommand: 'eks'
        awsSubCommand: 'update-kubeconfig'
        awsArguments: '--name $(APP_NAME)-cluster --region $(AWS_REGION)' 

    - script: |
        kubectl get ingress -n $(APP_NAME) $(kubectl get ingress -n $(APP_NAME) | awk 'NR>1{print $1}') -o=jsonpath="{'http://'}{.status.loadBalancer.ingress[].hostname}{'\n'}" > my-app-url.txt
      displayName: 'Save URL'

    - task: PublishBuildArtifacts@1
      displayName: 'Export URL'
      inputs:
        PathtoPublish: 'my-app-url.txt'
        ArtifactName: 'URL'
        publishLocation: 'Container' 
        



- stage: DeployProd
  condition: succeeded()
  # dependsOn: DeployDev
  jobs:
  - job: ApproveRelease
    timeOutInMinutes: 1440 # Job times out in 24hs
    pool: server
    steps:
    - task: ManualValidation@0
      inputs:
        notifyUsers: '$(USER_EMAIL)'
        instructions: 'Please validate and approve deployment to prod.'

  - job: DeployProdJob
    displayName: 'Deploy Prod'
    steps:

    # In this case it's necessary to specify the checkout with the persistCredential options set to true. This will enable us to push the changes to the repo.
    - checkout: self
      persistCredentials: true

    - script: |
        sed 's/tag:.*/tag: $(Build.BuildId)/g' helm/my-app/values.yaml > helm/my-app/values.temp
        mv helm/my-app/values.temp helm/my-app/values.yaml
      displayName: Update Tag In values.yaml

    - script: |
        git config --global user.email "AzureDevOps@Build&DeployAppPipeline.com"
        git config --global user.name "Azure DevOps - Build & Deploy App Pipeline"
      displayName: 'Configure Git user'

    - script: |
        git checkout -b main
        git add helm/my-app/values.yaml 
        git commit -m "App version tag updated to $(Build.BuildId) by Azure DevOps"
        git push --set-upstream origin main
      displayName: 'Push changes to GitHub'

    - script: |
        mkdir ~/.aws
        echo -e "[default]\naws_access_key_id = $(aws_access_key_id)\naws_secret_access_key = $(aws_secret_access_key)" > ~/.aws/credentials
        echo -e "[default]\nregion = $(AWS_REGION)"> ~/.aws/config 
      displayName: 'Configure AWS Profile'

    - script: |
        echo "false" > is-active.txt
        while [ "$(cat is-active.txt)" != "true" ]; do
            sleep 5
            aws elbv2 describe-load-balancers > lb-status.json
            json_data=$(cat lb-status.json)
            state_code=$(echo "$json_data" | jq -r '.LoadBalancers[1].State.Code')
            if [[ "$state_code" == "active" ]]; then
              echo "true" > is-active.txt
            else
              echo "Load balancer is not ready yet..."
            fi
        done
      displayName: 'Wait for Load Balancer Status'

    - task: AWSCLI@1
      displayName: 'Update KubeConfig'
      inputs:
        awsCredentials: 'aws'
        regionName: '$(AWS_REGION)' 
        awsCommand: 'eks'
        awsSubCommand: 'update-kubeconfig'
        awsArguments: '--name $(APP_NAME)-cluster --region $(AWS_REGION)' 

    - script: |
        kubectl get ingress -n $(APP_NAME) $(kubectl get ingress -n $(APP_NAME) | awk 'NR>1{print $1}') -o=jsonpath="{'http://'}{.status.loadBalancer.ingress[].hostname}{'\n'}" > my-app-url.txt
      displayName: 'Save URL'

    - task: PublishBuildArtifacts@1
      displayName: 'Export URL'
      inputs:
        PathtoPublish: 'my-app-url.txt'
        ArtifactName: 'URL'
        publishLocation: 'Container'         
